"""script to aggregate binary .dat files generated by running something like
the following:
```bash
find evaluationTraces/ -iname 'SHORT_*.gz' | xargs -n 1 ./simnlog
```
'traces' contains the extract training or evaluation traces from CBP-16. Sub.
SHORT_ for whatever type of trace you'd like to extract, but note that large
files will be generated.
"""
import os
import re
import warnings
from glob import glob
from collections import defaultdict

import pandas as pd
import numpy as np

try:
    from tqdm.auto import tqdm
except ImportError:
    warnings.warn('tqdm not installed, using dummy progress bar instead')


    class tqdm:
        def __init__(self, *args, **kwargs): pass

        def update(self, *args, **kwargs): pass

        def close(self): pass

        def set_description(self, *args, **kwargs): pass

# The maximum number of bytes to read in at a time (500 MB right now)
BATCH_BYTES = 1024 * 1024 * 512

DAT_EXT = '.dat'
# TODO: you change between 'traces' and 'evaluationTraces' directories
DAT_PATH = os.path.join(os.path.dirname(__file__), '..',
                        'cbp2016.eval', 'evaluationTraces')
# Binary files are generated from TAGE 64KB:
#   submissions/AndreSeznecLimited/cbp64KB
# TODO: very hard-coded - change AndreSeznec_cbp64KB to the submission and BPU
#  size that you would like to (not that these results only match up with those
#  from evaluationTraces and not the traces directory
EVL_PATH = os.path.join(os.path.dirname(__file__), '..',
                        'cbp2016.eval', 'results',
                        'cbp2016_evaluation_results',
                        'AndreSeznec_cbp64KB')

RES_PATH = os.path.join(os.path.dirname(__file__), '..',
                        'processed_traces')

# Create dtype used to store BPU and trace outputs
names = 'branchTaken', 'predDir', 'conditional', 'opType', 'branchTarget', 'PC'
formats = 'u1', 'u1', 'u1', 'u4', 'u8', 'u8'
offsets = 0, 1, 2, 4, 8, 16
bpu_dtype = np.dtype(dict(names=names, formats=formats, offsets=offsets))

RES_REGEX = re.compile(r'NUM_CONDITIONAL_BR\s*:\s*(?P<n_br>\d+).*'
                       r'NUM_MISPREDICTIONS\s*:\s*(?P<missed>\d+)')


def parse_eval_res_acc(eval_res_filename):
    # TODO: note this does subset of things in analyze_cbp16 but potentially
    #  more efficiently...
    with open(eval_res_filename, 'r') as f:
        txt = f.read()
    m = RES_REGEX.search(txt)
    # Returns accuracy at conditional branch level
    n_missed = int(m.group('missed'))
    n_branches = int(m.group('n_br'))
    return 1. - n_missed / n_branches, n_branches - n_missed


OPTYPE_ERR_STR = 'Unexpected non-unique opTypes for PC {}: {}'
WARMUP_PCTS = [.2, .4, .5, .6, .7, .8, .85, .9, .95, .99]


def process_trace(filename, eval_res_filename):
    # Read in items on a boundary of bpu_dtype-type items
    batch_size = round(BATCH_BYTES / bpu_dtype.itemsize) * bpu_dtype.itemsize
    n_branches = 0
    n_branches_uncond = 0

    # Parse eval file for conditional branch accuracy
    # TODO: APPARENTLY THESE WERE FOR EVAL TRACES AND NOT TRAIN TRACES, FILE
    #  NAMES ARE SAME BUT TRACES DIFFER.... -_- I'm dumb
    # pred_acc, pred_correct_tot = parse_eval_res_acc(eval_res_filename)

    filesize = os.stat(filename).st_size
    trace_name = os.path.basename(filename).split('.', 1)[0]
    n_branches_expect_ = filesize / bpu_dtype.itemsize
    n_branches_expect = int(n_branches_expect_)
    if n_branches_expect != n_branches_expect_:
        raise RuntimeError(
            '{} does not have a valid byte total. File size {} is not '
            'divisible by BPU data struct size of {}.'.format(
                filename, filesize, bpu_dtype.itemsize)
        )

    # Maps PCs to aggregated data
    # @formatter:off
    agg_data = defaultdict(lambda: [
        np.uint64(0),  # True positive count
        np.uint64(0),  # False positive count
        np.uint64(0),  # True negative count
        np.uint64(0),  # False negative count
        None,          # OpType (should be stored as np.uint8)
        np.uint64(0),  # Transition Count (for computing transition rate)
    ])
    # @formatter:on

    # entry per warm-up pct metric
    warmup_counts = [np.uint64(0)] * len(WARMUP_PCTS)
    prev_acc = 0.

    print('Processing', trace_name,
          '({} Bytes = {} Branch Instructions)'.format(
              filesize, n_branches_expect))
    # TODO: overlap ends of batches if doing sequence-like metrics
    with open(filename, 'rb') as f:
        pbar = tqdm(desc=trace_name, total=filesize, unit='B', unit_scale=True)
        while True:
            # Read the file in batches
            batch = f.read(batch_size)
            if not batch:
                break
            # Python bytes to NumPy array with bpu_dtype
            a = np.frombuffer(batch, bpu_dtype)
            pbar.update(a.nbytes)
            # To Pandas DataFrame
            df = pd.DataFrame(a)
            n_branches_batch = len(df)

            # Only parse conditional branches (drop from df)
            pbar.set_description(trace_name + ' conditionality')
            uncond = df[~df.conditional.astype(bool)].index
            n_branches_uncond_batch = len(uncond)
            df.drop(uncond, inplace=True)
            df.reset_index(drop=True, inplace=True)

            # Calculate warm-up time metrics - do so before updating counts of
            # branches
            n_branches_cond = n_branches - n_branches_uncond
            cum_acc = (  # @formatter:off
                    # Cumulative sum of count of correctly predicted takens
                    # including count of correct branches in previous batches
                    ((df.branchTaken.values == df.predDir.values).cumsum() +
                     n_branches_cond * prev_acc) /
                    # Cumulative count of conditional branches
                    (np.arange(1, len(df) + 1) + n_branches_cond)
            )  # @formatter:on
            prev_acc = cum_acc[-1]
            assert not (cum_acc > 1.).any()
            for i, pct in enumerate(WARMUP_PCTS):
                # min_acc = pred_acc * pct
                min_acc = pct
                br_count = np.argwhere(cum_acc < min_acc)
                if br_count.size:
                    # Index of last value where accuracy is less than pct of
                    # final accuracy achieved + 1 translates to number of
                    # conditional branches before predictor "warms up" to
                    # min_acc performance. Update results appropriately (account
                    # for cond. branches from previous batches)
                    warmup_counts[i] = np.uint64(
                        br_count[-1][0] + 1 + n_branches_cond
                    )
                    # if warmup_counts[i] > pred_correct_tot:
                    #     print(np.sort(cum_acc)[::-1])
                    #     raise ValueError('Why does God hate me',
                    #                      warmup_counts[i],
                    #                      pred_correct_tot,
                    #                      len(df))
                    # if warmup_counts[i] / pct > pred_correct_tot:
                    #     print(np.sort(cum_acc)[::-1])
                    #     raise ValueError('Why does God hate me too',
                    #                      warmup_counts[i],
                    #                      pct,
                    #                      warmup_counts[i] / pct,
                    #                      pred_correct_tot,
                    #                      len(df))

            # Now update the counts
            n_branches += n_branches_batch
            n_branches_uncond += n_branches_uncond_batch

            # Previous taken entries for each PC of the prior batch
            taken_prev = {}
            for PC, df_PC in df.groupby('PC'):
                pbar_str = 'Processing ' + hex(PC)

                # Confusion Matrix Counts
                pbar.set_description(pbar_str + ' confusion matrix')
                true_br = df_PC.branchTaken.values
                pred_br = df_PC.predDir.values

                tp = (true_br & pred_br).sum()
                fp = (~true_br & pred_br).sum()
                fn = (true_br & ~pred_br).sum()
                tn = len(df_PC) - (tp + fp + fn)

                # Update counts
                agg_data[PC][0] += np.uint64(tp)
                agg_data[PC][1] += np.uint64(fp)
                agg_data[PC][2] += np.uint64(tn)
                agg_data[PC][3] += np.uint64(fn)

                # PC OpType
                pbar.set_description(pbar_str + ' opType')
                opTypes = df_PC.opType.unique()
                if len(opTypes) != 1:  # check for uniqueness
                    raise RuntimeError(OPTYPE_ERR_STR.format(PC, list(opTypes)))
                if agg_data[PC][4] is None:
                    # Store opType for PC
                    agg_data[PC][4] = np.uint8(opTypes[0])
                elif agg_data[PC][4] != opTypes[0]:  # check for uniqueness
                    raise RuntimeError(OPTYPE_ERR_STR.format(
                        PC, [opTypes[0], agg_data[PC][4]]))

                # Transition counts (requires storing previous entry for each
                # PC)
                agg_data[PC][5] += np.uint64(
                    (true_br[1:] != true_br[:-1]).sum()
                )
                # Account for if PC was present in previous batch (only need
                # last value here for taken)
                maybe_pc_prev = taken_prev.get(PC)
                if maybe_pc_prev is not None:
                    agg_data[PC][5] += np.uint64(true_br[0] == maybe_pc_prev)
                taken_prev[PC] = true_br[-1]

            # End PC groupby for loop - Cleanup
            del a
        # End while True batch processing
        pbar.close()
    # File closed
    if n_branches_expect != n_branches:
        raise RuntimeError(
            'Error parsing {} - expected {} branches but instead found {} '
            'branches.'.format(filename, n_branches_expect, n_branches)
        )
    print('Aggregated stats on {}/{} branch instr ({} unconditional)'.format(
        n_branches - n_branches_uncond, n_branches, n_branches_uncond))
    return trace_name, agg_data, warmup_counts


def main():
    print('Processing', DAT_EXT, 'files from', DAT_PATH)

    if not os.path.exists(RES_PATH):  # Results directory
        os.mkdir(RES_PATH)

    to_process = glob(os.path.join(DAT_PATH, '*' + DAT_EXT))
    if not to_process:
        warnings.warn(
            'The directory {} contains no {} files - ensure you have the '
            'correct directory specified in this script (e.g. traces vs. '
            'evaluationTraces) and that you previously ran the ./simnlog '
            'program, (e.g. something like:\nfind evaluationTraces/ -iname '
            '\'SHORT_*.gz\' '
            '| xargs -n 1 ./simnlog'.format(DAT_PATH, DAT_EXT)
        )

    for filename in to_process:
        # Process the trace
        # TODO: hard-coded-ish
        n_trunc = len('.bt9.trace.gz') + len(DAT_EXT)
        eval_res_filename = os.path.join(
            EVL_PATH, os.path.basename(filename)[:-n_trunc] + '.res'
        )
        trace_name, trace_data, warmup_counts = process_trace(
            filename, eval_res_filename)

        ext = '.h5'
        save_path = os.path.join(RES_PATH, trace_name + ext)
        if os.path.exists(save_path):
            raise FileExistsError(
                save_path + ' already exists on disk, delete or move the file '
                            'for this script to continue')
        # Save the trace metrics as DataFrame
        df_res = pd.DataFrame(trace_data.values(), index=trace_data.keys(),
                              columns=['TP', 'FP', 'TN', 'FN', 'opType',
                                       'trans_count'])
        df_res.index.name = 'PC'
        df_res.to_hdf(save_path, 'df', mode='w')

        # Save scalars as series
        s_warmup = pd.Series(data=warmup_counts,
                             index=['warmup_{}pct'.format(int(100 * wp))
                                    for wp in WARMUP_PCTS])
        s_warmup.to_hdf(save_path, 's', mode='a')


if __name__ == '__main__':
    main()
